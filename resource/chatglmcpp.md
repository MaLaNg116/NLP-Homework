# ChatGLM3 chatglm.cppåŠ é€Ÿéƒ¨ç½²æŒ‡åŒ—

[![CMake](https://github.com/li-plus/chatglm.cpp/actions/workflows/cmake.yml/badge.svg)](https://github.com/li-plus/chatglm.cpp/actions/workflows/cmake.yml)
[![Python package](https://github.com/li-plus/chatglm.cpp/actions/workflows/python-package.yml/badge.svg)](https://github.com/li-plus/chatglm.cpp/actions/workflows/python-package.yml)
[![PyPI](https://img.shields.io/pypi/v/chatglm-cpp)](https://pypi.org/project/chatglm-cpp/)
![Python](https://img.shields.io/pypi/pyversions/chatglm-cpp)
[![License: MIT](https://img.shields.io/badge/license-MIT-blue)](LICENSE)

## ChatGLM.cppä»‹ç»

è¿™æ˜¯å¯¹ä¸€ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„C++å®ç°ï¼ŒåŒ…æ‹¬ [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B), [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B), [ChatGLM3-6B](https://github.com/THUDM/ChatGLM3) ç­‰ï¼Œç”¨äºåœ¨æ‚¨çš„ç¬”è®°æœ¬ä¸Šè¿›è¡Œå®æ—¶èŠå¤©ã€‚

![demo](./demo.gif)

## åŠŸèƒ½ç‰¹ç‚¹

äº®ç‚¹ï¼š
+ åŸºäº [ggml](https://github.com/ggerganov/ggml) çš„çº¯C++å®ç°ï¼Œå·¥ä½œæ–¹å¼ä¸ [llama.cpp](https://github.com/ggerganov/llama.cpp) ç›¸åŒã€‚
+ é€šè¿‡int4/int8é‡åŒ–ã€ä¼˜åŒ–çš„KVç¼“å­˜å’Œå¹¶è¡Œè®¡ç®—åŠ é€Ÿäº†å†…å­˜é«˜æ•ˆçš„CPUæ¨ç†ã€‚
+ æµå¼ç”Ÿæˆï¼Œæ‰“å­—æœºæ•ˆæœã€‚
+ æä¾›Pythonç»‘å®šã€ç½‘é¡µæ¼”ç¤ºã€APIæœåŠ¡å™¨ç­‰å¤šç§å¯èƒ½æ€§ã€‚

æ”¯æŒï¼š
+ ç¡¬ä»¶ï¼šx86/arm CPU, NVIDIA GPU, Apple Silicon GPU
+ å¹³å°ï¼šLinux, MacOS, Windows
+ æ¨¡å‹ï¼š[ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B), [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B), [ChatGLM3-6B](https://github.com/THUDM/ChatGLM3), [CodeGeeX2](https://github.com/THUDM/CodeGeeX2), [Baichuan-13B](https://github.com/baichuan-inc/Baichuan-13B), [Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B), [Baichuan-13B](https://github.com/baichuan-inc/Baichuan-13B), [Baichuan2](https://github.com/baichuan-inc/Baichuan2), [InternLM](https://github.com/InternLM/InternLM)

**æ³¨æ„**ï¼šBaichuan & InternLMæ¨¡å‹ç³»åˆ—å·²ä¸å†æ¨èä½¿ç”¨ï¼Œè½¬è€Œæ¨èä½¿ç”¨ [llama.cpp](https://github.com/ggerganov/llama.cpp)ã€‚

## éƒ¨ç½²æŒ‡åŒ—

**éƒ¨ç½²å‰å‡†å¤‡**

+ ç¡®ä¿ä½ å·²é€šè¿‡ [Microsoft Visual Studio](https://visualstudio.microsoft.com/zh-hans/downloads/) æ­£ç¡®é…ç½®æ¡Œé¢ C++ å¼€å‘ç¯å¢ƒ
+ ç¡®ä¿ä½ å·²æ­£ç¡®å®‰è£…é…ç½® [CMake](https://cmake.org/)
+ ç¡®ä¿ä½ å·²æ­£ç¡®å®‰è£… [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit)
+ ç¡®ä¿ä½ å·²æ­£ç¡®å®‰è£… [HuggingFace Cli](https://huggingface.co/docs/huggingface_hub/main/en/guides/cli)

:exclamation: :exclamation: :exclamation: å¦‚æœä½ å·²æ­£ç¡®å®Œæˆä»¥ä¸Šé…ç½®ï¼Œåœ¨ç¼–è¯‘æ–‡ä»¶æ—¶ CMake æŒ‡ä»¤ä»ç„¶æŠ¥å‘Šæ‰¾ä¸åˆ°CUDAç¯å¢ƒçš„é”™è¯¯ï¼Œå¯å‚è€ƒ [cmakeæŠ¥å‘Šæ‰¾ä¸åˆ°CUDAç¯å¢ƒ@Windows VC2022](https://blog.csdn.net/weixin_36829761/article/details/136896036) 

**é¡¹ç›®æ‹‰å–**

æ‹‰å– ChatGLM.cpp é¡¹ç›®åˆ°æœ¬åœ°
```sh
git clone --recursive https://github.com/li-plus/chatglm.cpp.git && cd chatglm.cpp
```
å¦‚æœåœ¨å…‹éš†ä»“åº“æ—¶å¿˜è®°äº† --recursive æ ‡å¿—ï¼Œè¯·åœ¨ chatglm.cpp æ–‡ä»¶å¤¹ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```sh
git submodule update --init --recursive
```

:exclamation: :exclamation: :exclamation: å¦‚æœä½ åœ¨æ‹‰å–é¡¹ç›®æ—¶é‡åˆ°äº†é—®é¢˜ï¼Œå¯å°è¯•ï¼š
+ æä¾›æ›´ä¸ºç¨³å®šçš„å›½é™…ç½‘ç»œè®¿é—®ä½“éªŒ
+ å°è¯•ä½¿ç”¨ SSH ä»£æ›¿ HTTP æ‹‰å–
+ å¦‚æœæ˜¯ [Git clone fetch-pack unexpected disconnect while reading sideband packet](https://blog.csdn.net/m0_37196398/article/details/129993150) é”™è¯¯ï¼Œå‚è€ƒåšå®¢

**æ¨¡å‹é‡åŒ–**

ç”±äºé‡åŒ–çš„è¿‡ç¨‹æ¯”è¾ƒç¼“æ…¢ï¼Œä¸”æœ‰å¯èƒ½é‡åŒ–å¤±è´¥ï¼Œæ‰€ä»¥æˆ‘ä»¬æ­¤æ¬¡éƒ¨ç½²é‡‡å–ç›´æ¥ä»HuggingFaceæ‹‰å–ä»–äººé‡åŒ–å¥½çš„æ¨¡å‹ï¼Œæ­¤å¤„æˆ‘é€‰æ‹©çš„æ¨¡å‹æ˜¯ `kingzzm/chatglm3-6b-ggml`
```sh
huggingface-cli download --resume-download kingzzm/chatglm3-6b-ggml --local-dir chatglm3-6b-ggml
```

:exclamation: ä¸‹è½½å¥½åè¯·å°†æ¨¡å‹åå­—æ”¹æˆ `chatglm-ggml.bin ` (æˆ–è€…å…¶ä»–ä½ æƒ³æ”¹çš„åå­—ï¼Œæ­¤å¤„ä»…æ˜¯ä¸ºäº†åé¢æ–¹ä¾¿ copy å‘½ä»¤)

å¦‚æœä½ æƒ³è¦è‡ªè¡Œé‡åŒ–æ¨¡å‹ï¼Œå‚è€ƒ <a href="#:books:æ¨¡å‹é‡åŒ–">:books:æ¨¡å‹é‡åŒ–</a>

**ç¼–è¯‘**

ç”¨ CMake æŒ‡ä»¤ç¼–è¯‘æºç ï¼š
```sh
cmake -B build
cmake --build build -j --config Release
```
æ­¤ç¼–è¯‘ç‰ˆæœ¬ä¸º CPU ç‰ˆæœ¬ï¼Œå¦‚æœä½ éœ€è¦ CUDA åŠ é€Ÿï¼Œåˆ™éœ€è¦ä½¿ç”¨ cuBLAS åŠ é€Ÿæ¡†æ¶ï¼š
```sh
cmake -B build -DGGML_CUBLAS=ON && cmake --build build -j
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å†…æ ¸éƒ½å°†ä¸ºæ‰€æœ‰å¯èƒ½çš„CUDAæ¶æ„ç¼–è¯‘ï¼Œè¿™éœ€è¦ä¸€äº›æ—¶é—´ã€‚è¦åœ¨ç‰¹å®šç±»å‹çš„è®¾å¤‡ä¸Šè¿è¡Œï¼Œæ‚¨å¯ä»¥æŒ‡å®š `CUDA_ARCHITECTURES` ä»¥åŠ å¿«nvccç¼–è¯‘çš„é€Ÿåº¦ã€‚ä¾‹å¦‚ï¼š
```sh
cmake -B build -DGGML_CUBLAS=ON -DCUDA_ARCHITECTURES="80"       # for A100
cmake -B build -DGGML_CUBLAS=ON -DCUDA_ARCHITECTURES="70;75"    # compatible with both V100 and T4
```

è¦æ‰¾å‡ºæ‚¨çš„GPUè®¾å¤‡çš„CUDAæ¶æ„ï¼Œè¯·å‚é˜… [Your GPU Compute Capability](https://developer.nvidia.com/cuda-gpus)ã€‚

**å¯åŠ¨**

ç°åœ¨ä½ å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤å’Œé‡åŒ–åçš„ChatGLM3-6Bæ¨¡å‹å¯¹è¯ï¼š
```sh
./build/bin/Release/main -m chatglm-ggml.bin -p ä½ å¥½
# ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
```

è¦ä»¥äº¤äº’æ¨¡å¼è¿è¡Œæ¨¡å‹ï¼Œè¯·æ·»åŠ  -i æ ‡å¿—ã€‚ä¾‹å¦‚ï¼š
```sh
./build/Release/bin/main -m chatglm-ggml.bin -i
```
åœ¨äº¤äº’æ¨¡å¼ä¸‹ï¼Œæ‚¨çš„èŠå¤©å†å²å°†ä½œä¸ºä¸‹ä¸€è½®å¯¹è¯çš„ä¸Šä¸‹æ–‡ã€‚

è¿è¡Œ `./build/bin/main -h` æ¥æ¢ç´¢æ›´å¤šé€‰é¡¹ï¼

### API æœåŠ¡

æ”¯æŒå„ç§APIæœåŠ¡å™¨ä»¥ä¸æµè¡Œçš„å‰ç«¯é›†æˆã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…é¢å¤–çš„ä¾èµ–
```sh
pip install chatglm-cpp[api]
```
è®°å¾—æ·»åŠ ç›¸åº”çš„ `CMAKE_ARGS` ä»¥å¯ç”¨åŠ é€Ÿã€‚

**LangChain API**

å¯åŠ¨ä¸€ä¸ª LangChain API æœåŠ¡å™¨ï¼š
```sh
MODEL=./chatglm2-ggml.bin uvicorn chatglm_cpp.langchain_api:app --host 127.0.0.1 --port 6006
```

ä½¿ç”¨ `curl` å‘½ä»¤æµ‹è¯•ä½ çš„ç«¯ç‚¹ï¼š
```sh
curl http://127.0.0.1:6006 -H 'Content-Type: application/json' -d '{"prompt": "ä½ å¥½"}'
```

ä½¿ç”¨ `LangChain` è¿è¡Œ:
```python
>>> from langchain.llms import ChatGLM
>>> 
>>> llm = ChatGLM(endpoint_url="http://127.0.0.1:8000")
>>> llm.predict("ä½ å¥½")
'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'
```

**OpenAI API**

å¯åŠ¨ä¸€ä¸ªä¸ [OpenAIèŠå¤©è¡¥å…¨åè®®](https://platform.openai.com/docs/api-reference/chat) å…¼å®¹çš„APIæœåŠ¡å™¨ï¼š
```sh
MODEL=./chatglm3-ggml.bin uvicorn chatglm_cpp.openai_api:app --host 127.0.0.1 --port 6006
```

ä½¿ç”¨ `curl` å‘½ä»¤æµ‹è¯•ä½ çš„ç«¯ç‚¹ï¼š
```sh
curl http://127.0.0.1:6006/v1/chat/completions -H 'Content-Type: application/json' \
    -d '{"messages": [{"role": "user", "content": "ä½ å¥½"}]}'
```

ä½¿ç”¨ `OpenAI client` å’Œä½ çš„æ¨¡å‹èŠå¤©:
```python
>>> from openai import OpenAI
>>> 
>>> client = OpenAI(base_url="http://127.0.0.1:6006/v1")
>>> response = client.chat.completions.create(model="default-model", messages=[{"role": "user", "content": "ä½ å¥½"}])
>>> response.choices[0].message.content
'ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚'
```

è‡³æ­¤ï¼Œå¯åˆ©ç”¨ `RemoteLLM` ç±»ï¼Œé€šè¿‡æœ¬åœ° `6006` ç«¯å£ä¸ä½ çš„æ¨¡å‹èŠå¤©ã€‚

å¦‚æœè¦ä½¿ç”¨æµå¼å›å¤ï¼Œå‚è€ƒä»¥ä¸‹è„šæœ¬ï¼š
```sh
OPENAI_BASE_URL=http://127.0.0.1:6006/v1 python3 examples/openai_client.py --stream --prompt ä½ å¥½
```

æ”¯æŒ `Tool calling` ï¼š
```sh
OPENAI_BASE_URL=http://127.0.0.1:6006/v1 python3 examples/openai_client.py --tool_call --prompt ä¸Šæµ·å¤©æ°”æ€ä¹ˆæ ·
```

ä½¿ç”¨è¿™ä¸ªAPIæœåŠ¡å™¨ä½œä¸ºåç«¯ï¼ŒChatGLM.cppæ¨¡å‹å¯ä»¥æ— ç¼é›†æˆåˆ°ä»»ä½•ä½¿ç”¨OpenAIé£æ ¼APIçš„å‰ç«¯ä¸­ï¼ŒåŒ…æ‹¬ [mckaywrigley/chatbot-ui](https://github.com/mckaywrigley/chatbot-ui), [fuergaosi233/wechat-chatgpt](https://github.com/fuergaosi233/wechat-chatgpt), [Yidadaa/ChatGPT-Next-Web](https://github.com/Yidadaa/ChatGPT-Next-Web)ï¼Œç­‰ç­‰ã€‚


### :books:æ¨¡å‹é‡åŒ–

å®‰è£…ç”¨äºåŠ è½½å’Œé‡åŒ–Hugging Faceæ¨¡å‹çš„å¿…è¦åŒ…ï¼š
```sh
python3 -m pip install -U pip
python3 -m pip install torch tabulate tqdm transformers accelerate sentencepiece
```

ä½¿ç”¨ `convert.py` å°† ChatGLM-6B è½¬æ¢ä¸ºé‡åŒ–åçš„ GGML æ ¼å¼ã€‚ä¾‹å¦‚ï¼Œå°† fp16 æ ¼å¼çš„åŸå§‹æ¨¡å‹è½¬æ¢ä¸º q4_0ï¼ˆé‡åŒ–ä¸ºint4ï¼‰çš„GGMLæ¨¡å‹ï¼Œè¿è¡Œï¼š
```sh
python3 chatglm_cpp/convert.py -i THUDM/chatglm-6b -t q4_0 -o chatglm-ggml.bin
```

åŸå§‹æ¨¡å‹ï¼ˆ `-i <model_name_or_path>` ï¼‰å¯ä»¥æ˜¯ Hugging Face æ¨¡å‹åç§°æˆ–æ‚¨é¢„å…ˆä¸‹è½½çš„æ¨¡å‹çš„æœ¬åœ°è·¯å¾„ã€‚ç›®å‰æ”¯æŒä»¥ä¸‹æ¨¡å‹:
* ChatGLM-6B: `THUDM/chatglm-6b`, `THUDM/chatglm-6b-int8`, `THUDM/chatglm-6b-int4`
* ChatGLM2-6B: `THUDM/chatglm2-6b`, `THUDM/chatglm2-6b-int4`
* ChatGLM3-6B: `THUDM/chatglm3-6b`
* CodeGeeX2: `THUDM/codegeex2-6b`, `THUDM/codegeex2-6b-int4`
* Baichuan & Baichuan2: `baichuan-inc/Baichuan-13B-Chat`, `baichuan-inc/Baichuan2-7B-Chat`, `baichuan-inc/Baichuan2-13B-Chat`

æ‚¨å¯ä»¥é€šè¿‡æŒ‡å®š `-t <type>` å°è¯•ä»¥ä¸‹ä»»ä½•é‡åŒ–ç±»å‹ï¼š
* `q4_0`: 4-bit integer quantization with fp16 scales.
* `q4_1`: 4-bit integer quantization with fp16 scales and minimum values.
* `q5_0`: 5-bit integer quantization with fp16 scales.
* `q5_1`: 5-bit integer quantization with fp16 scales and minimum values.
* `q8_0`: 8-bit integer quantization with fp16 scales.
* `f16`: half precision floating point weights without quantization.
* `f32`: single precision floating point weights without quantization.

å¯¹äºLoRAæ¨¡å‹ï¼Œè¯·æ·»åŠ  `-l <lora_model_name_or_path>` æ ‡å¿—ä»¥å°†æ‚¨çš„LoRAæƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ã€‚

### æ›´å¤šéƒ¨ç½²æ–¹æ³•å‚è€ƒ [ChatGLM.cppå®˜æ–¹ä»“åº“](https://github.com/li-plus/chatglm.cpp/tree/main)